{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d943f81b-2d13-4cdd-af92-be5f63a73b61",
   "metadata": {},
   "source": [
    "# モジュールのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb3fabde-7f62-4b5c-a838-6435b84f1cdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T11:26:30.715368Z",
     "iopub.status.busy": "2026-02-08T11:26:30.715047Z",
     "iopub.status.idle": "2026-02-08T11:26:31.023007Z",
     "shell.execute_reply": "2026-02-08T11:26:31.021662Z",
     "shell.execute_reply.started": "2026-02-08T11:26:30.715335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "[20:26:31] [INFO]: 必要なモジュールの読み込みが完了しました。\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ライブラリのインポート\n",
    "# ==========================================\n",
    "# 標準ライブラリのインポート\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pandas\n",
    "\n",
    "# プロジェクトルートを検索パスに追加（lib/ をパッケージとして認識させるため）\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "\n",
    "# 自作ライブラリのインポート\n",
    "from lib import data_loader\n",
    "from lib import logger_config\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] [INFO]: 必要なモジュールの読み込みが完了しました。\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a58e1a-38ff-429e-ac19-1bf58525c1df",
   "metadata": {},
   "source": [
    "# ユーザー事前定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47cc4400-2e49-4a5a-89f3-920e6419aab5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T11:26:31.024196Z",
     "iopub.status.busy": "2026-02-08T11:26:31.023846Z",
     "iopub.status.idle": "2026-02-08T11:26:31.033985Z",
     "shell.execute_reply": "2026-02-08T11:26:31.032663Z",
     "shell.execute_reply.started": "2026-02-08T11:26:31.024166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "[20:26:31] [INFO]: パラメータ定義をロードしました。\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ユーザー設定 (CONFIG)\n",
    "# ==========================================\n",
    "# DIRS:          データの読み書きを行うフォルダの場所を指定します。\n",
    "#   - BASE:      実行の基準となるフォルダ（通常は現在の場所）。\n",
    "#   - RAW:       元データ（CSV）が届く場所。\n",
    "#   - INTERIM:   日付やIDを整えた「クレンジング済み」の一時ファイルを置く場所。\n",
    "#   - PROCESSED: 単位変換（Mbps計算）などを終えたファイルを置く場所。\n",
    "#   - OUT:       全データを合体させた「最終アウトプット」を出す場所。\n",
    "#\n",
    "# KEYS:          異なるデータを合体させる際に「共通の目印」とする列の名前です。\n",
    "#\n",
    "# DEVICES:       機器ごとのCSVのクセを調整するための設定リストです。\n",
    "#   - id:        機器を識別するための内部用の名前。\n",
    "#   - pattern:   対象となるCSVファイルを探すための名前のルール。\n",
    "#   - read_opts: CSV読み込み時の特殊ルール（コメント行の無視など）。\n",
    "#   - clean:     「クレンジング」指示。日付形式の統一やIDの合体方法を決めます。\n",
    "#   - renames:   列の名前を日本語からプログラム用の英語に変換するマップ。\n",
    "#   - mbps:      計算（ByteからMbps）を行う対象の列を指定します。\n",
    "# ==========================================\n",
    "PROJECT_CONFIG = {\n",
    "    \"DIRS\": {\n",
    "        \"BASE\": os.getcwd(),\n",
    "        \"RAW\": \"./data/raw\",\n",
    "        \"INTERIM\": \"./data/interim\",\n",
    "        \"PROCESSED\": \"./data/processed\",\n",
    "        \"OUT\": \"./result\"\n",
    "    },\n",
    "    \"KEYS\": [\"timestamp\", \"line_id\"],\n",
    "    \"LOG_FILE_NAME\": \"execution.log\",\n",
    "    \"DEVICES\": [\n",
    "        {\n",
    "            \"id\": \"dev01\",\n",
    "            \"pattern\": \"*traffic-01*.csv\",\n",
    "            \"renames\": {\n",
    "                \"タイムスタンプ\": \"timestamp\", \n",
    "                \"回線番号\": \"line_id\",\n",
    "                \"下りのトラヒック量(Byte)\": \"dev01_down_Byte\",\n",
    "                \"上りのトラヒック量(Byte)\": \"dev01_up_Byte\",\n",
    "                \"下りの廃棄量(Byte)\": \"dev01_drop_Byte\"\n",
    "            },\n",
    "            \"mbps\": {\n",
    "                \"dev01_down_Byte\": \"dev01_down_Mbps\",\n",
    "                \"dev01_up_Byte\": \"dev01_up_Mbps\",\n",
    "                \"dev01_drop_Byte\": \"dev01_drop_Mbps\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"dev02\",\n",
    "            \"pattern\": \"sample-traffic-02.csv\",\n",
    "            \"read_opts\": {\"comment\": \"#\", \"quotechar\": '\"'},\n",
    "            \"clean\": {\n",
    "                \"date\": {\"col\": \"timestamp\", \"fmt\": \"%Y%m%d%H%M%S\"},\n",
    "                \"id\": [\"code\", \"identifer\"]\n",
    "            },\n",
    "            \"renames\": {\n",
    "                \"volume_in\": \"dev02_down_Byte\",\n",
    "                \"volume_out\": \"dev02_up_Byte\"\n",
    "            },\n",
    "            \"mbps\": {\n",
    "                \"dev02_down_Byte\": \"dev02_down_Mbps\",\n",
    "                \"dev02_up_Byte\": \"dev02_up_Mbps\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "APP_NAME = \"CREATE_MASTER-DATA\"\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] [INFO]: パラメータ定義をロードしました。\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a80f54-a600-4cb7-aad9-debdd410f300",
   "metadata": {},
   "source": [
    "# マスターデータ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fa2aa9b-4ca8-488b-a0e2-41d973a9c479",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T11:26:31.035281Z",
     "iopub.status.busy": "2026-02-08T11:26:31.034907Z",
     "iopub.status.idle": "2026-02-08T11:26:31.434648Z",
     "shell.execute_reply": "2026-02-08T11:26:31.433566Z",
     "shell.execute_reply.started": "2026-02-08T11:26:31.035251Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 20:26:31 [INFO] [CREATE_MASTER-DATA] === Pipeline Execution Started (Full Logic) ===\n",
      "2026-02-08 20:26:31 [INFO] [CREATE_MASTER-DATA] Final Report created: ./result/master_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Success\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CSVをクレンジング（掃除）・加工・結合し、マスターデータ作成\n",
    "# ==========================================\n",
    "\n",
    "def execute_pipeline(conf):\n",
    "    \"\"\"\n",
    "    データ処理のメイン工程（パイプライン）を実行する関数。\n",
    "    \n",
    "    Args:\n",
    "        conf (dict): フォルダの場所やデバイスごとの設定が入った辞書。\n",
    "    Returns:\n",
    "        str: 成功したら \"Success\"、失敗したらエラー内容を返す。\n",
    "    \"\"\"\n",
    "    # 設定ファイルから「フォルダの場所(dirs)」と「結合の目印(keys)」を取り出す\n",
    "    dirs = conf[\"DIRS\"]\n",
    "    keys = conf[\"KEYS\"]\n",
    "    \n",
    "    # 処理の記録を付けるための「ロガー」を準備（どこに保存するかなどを指定）\n",
    "    logger = logger_config.setup_logger(\n",
    "        APP_NAME, \n",
    "        dirs.get(\"BASE\"), \n",
    "        file_name=conf.get(\"LOG_FILE_NAME\")\n",
    "    )\n",
    "    \n",
    "    logger.info(\"=== Pipeline Execution Started (Full Logic) ===\")\n",
    "\n",
    "    try:\n",
    "        # --------------------------------------------------\n",
    "        # Step 1: Cleaning (生データの読み込みとお掃除)\n",
    "        # --------------------------------------------------\n",
    "        # 設定にあるデバイスの数だけ、順番に処理を繰り返す\n",
    "        for dev in conf[\"DEVICES\"]:\n",
    "            # 生データ(RAW)フォルダから、指定されたパターンのCSVファイルを読み込む\n",
    "            df = data_loader.load_csv(dirs[\"RAW\"], dev[\"pattern\"], dev.get(\"read_opts\"))\n",
    "            if df is None:\n",
    "                continue # ファイルが見つからない場合は次のデバイスへ\n",
    "            \n",
    "            # 「clean」の設定がある場合、日付やIDを使いやすい形に整える\n",
    "            if dev.get(\"clean\"):\n",
    "                # 日付変換の設定を取得\n",
    "                d_cfg = dev[\"clean\"].get(\"date\")\n",
    "                if d_cfg:\n",
    "                    # 日付列を文字列から日時の形式（datetime）に変換した後、指定の形式（YYYY-MM-DD HH:MM:SS）の文字列に整える\n",
    "                    df[d_cfg['col']] = pandas.to_datetime(df[d_cfg['col']], format=d_cfg['fmt']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                \n",
    "                # ID（回線を識別する名前）を作る設定を取得\n",
    "                id_p = dev[\"clean\"].get(\"id\")\n",
    "                if id_p:\n",
    "                    # 指定された2つの列を文字列として取得し、ハイフンで繋いで新しいID列「line_id」を作成する\n",
    "                    df['line_id'] = df[id_p[0]].astype(str) + \"-\" + df[id_p[1]].astype(str)\n",
    "            \n",
    "            # クレンジングが終わったデータを、一時保存用の「INTERIM」フォルダに保存する\n",
    "            data_loader.save_csv(df, os.path.join(dirs[\"INTERIM\"], f\"{dev['id']}_cleaned.csv\"))\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # Step 2: Processing (リネームとMbps計算)\n",
    "        # --------------------------------------------------\n",
    "        # お掃除が終わったファイルを読み込み、計算や列名の変更を行う\n",
    "        for dev in conf[\"DEVICES\"]:\n",
    "            in_path = os.path.join(dirs[\"INTERIM\"], f\"{dev['id']}_cleaned.csv\")\n",
    "            if not os.path.exists(in_path):\n",
    "                continue\n",
    "            \n",
    "            # 保存した一時ファイルを読み直す\n",
    "            df = pandas.read_csv(in_path)\n",
    "            \n",
    "            # 設定（CONFIG）で指定した対応表に従って、列名を日本語から英語などへ変更する\n",
    "            df = df.rename(columns=dev[\"renames\"])\n",
    "            \n",
    "            # 通信量の単位を「Byte」から「Mbps（速度）」に変換する計算\n",
    "            m_map = dev.get(\"mbps\", {})\n",
    "            for b_col, m_col in m_map.items():\n",
    "                if b_col in df.columns:\n",
    "                    # 通信速度の計算式: (バイト数 * 8ビット) / (5分間の秒数 * 100万)\n",
    "                    df[m_col] = ((df[b_col] * 8) / (300 * 1000000)).round(1)\n",
    "            \n",
    "            # 結合キー、変更後の列名、計算したMbps列を合算して「必要な列リスト」を作成する\n",
    "            req = keys + list(dev[\"renames\"].values()) + list(m_map.values())\n",
    "            # 実際にデータフレーム内に存在している列だけを抽出し、不足エラーを防ぐ\n",
    "            actual = [c for c in req if c in df.columns]\n",
    "            # 必要な列だけに絞り込んだデータを、加工済みフォルダに保存する\n",
    "            data_loader.save_csv(df[actual], os.path.join(dirs[\"PROCESSED\"], f\"{dev['id']}_processed.csv\"))\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # Step 3: Merging (全デバイスのデータを合体)\n",
    "        # --------------------------------------------------\n",
    "        # バラバラだった各デバイスの表を、時間とIDを基準に「横」に繋げる\n",
    "        merged_df = None\n",
    "        for dev in conf[\"DEVICES\"]:\n",
    "            p_path = os.path.join(dirs[\"PROCESSED\"], f\"{dev['id']}_processed.csv\")\n",
    "            if not os.path.exists(p_path):\n",
    "                continue\n",
    "            \n",
    "            df = pandas.read_csv(p_path)\n",
    "            # 最初の1台目のデータなら、それを合体データの土台にする\n",
    "            if merged_df is None:\n",
    "                merged_df = df\n",
    "            else:\n",
    "                # 2台目以降は、時間(timestamp)とID(line_id)が一致する場所で横に合体させる\n",
    "                merged_df = pandas.merge(merged_df, df, on=keys, how='outer').fillna(0)\n",
    "\n",
    "        # 全ての合体が無事に終わっていたら、最後の仕上げをする\n",
    "        if merged_df is not None:\n",
    "            # 「_Byte」で終わる名前の列（通信量）を、小数点のない「整数型(int)」に変換して見やすくする\n",
    "            for c in [c for c in merged_df.columns if c.endswith('_Byte')]:\n",
    "                merged_df[c] = merged_df[c].astype(int)\n",
    "            \n",
    "            # 最終的なマスターデータを「OUT」フォルダに書き出す\n",
    "            final_path = os.path.join(dirs[\"OUT\"], \"master_data.csv\")\n",
    "            data_loader.save_csv(merged_df, final_path)\n",
    "            logger.info(f\"Final Report created: {final_path}\")\n",
    "            return \"Success\"\n",
    "        \n",
    "        return \"Error: No data to merge\"\n",
    "\n",
    "    # もし途中で予期せぬエラー（ファイルが壊れている等）が起きたら、ログに記録して中断する\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Critical Pipeline Error: {str(e)}\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# ==========================================\n",
    "# 実行エントリーポイント\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    result = execute_pipeline(PROJECT_CONFIG)\n",
    "    print(f\"Status: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bac7f73-77ce-4bd5-aaf7-5c5b1d49b674",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
